{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81ff4e80",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/space/hotel/bachn/miniconda3/envs/llm/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "\n",
    "HUGGINGFACE_TOKEN = ''\n",
    "\n",
    "login(token=HUGGINGFACE_TOKEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "badfbafc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "comet_ml version 3.43.1 is installed, but version 3.43.2 or higher is required. Please update comet_ml to the latest version to enable Comet logging with pip install 'comet-ml>=3.43.2'.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "\n",
    "from peft import PeftModel, PeftConfig, LoraConfig, TaskType, get_peft_model, get_peft_config\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from transformers import DataCollatorForLanguageModeling, Trainer, TrainingArguments\n",
    "from datasets import load_dataset\n",
    "from trl import SFTTrainer\n",
    "\n",
    "base_model_id = \"google/gemma-3-1b-it\"\n",
    "cache_dir = \"./cache\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3fdb192c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Gemma3ForCausalLM(\n",
       "  (model): Gemma3TextModel(\n",
       "    (embed_tokens): Gemma3TextScaledWordEmbedding(262144, 1152, padding_idx=0)\n",
       "    (layers): ModuleList(\n",
       "      (0-25): 26 x Gemma3DecoderLayer(\n",
       "        (self_attn): Gemma3Attention(\n",
       "          (q_proj): Linear(in_features=1152, out_features=1024, bias=False)\n",
       "          (k_proj): Linear(in_features=1152, out_features=256, bias=False)\n",
       "          (v_proj): Linear(in_features=1152, out_features=256, bias=False)\n",
       "          (o_proj): Linear(in_features=1024, out_features=1152, bias=False)\n",
       "          (q_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
       "          (k_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
       "        )\n",
       "        (mlp): Gemma3MLP(\n",
       "          (gate_proj): Linear(in_features=1152, out_features=6912, bias=False)\n",
       "          (up_proj): Linear(in_features=1152, out_features=6912, bias=False)\n",
       "          (down_proj): Linear(in_features=6912, out_features=1152, bias=False)\n",
       "          (act_fn): PytorchGELUTanh()\n",
       "        )\n",
       "        (input_layernorm): Gemma3RMSNorm((1152,), eps=1e-06)\n",
       "        (post_attention_layernorm): Gemma3RMSNorm((1152,), eps=1e-06)\n",
       "        (pre_feedforward_layernorm): Gemma3RMSNorm((1152,), eps=1e-06)\n",
       "        (post_feedforward_layernorm): Gemma3RMSNorm((1152,), eps=1e-06)\n",
       "      )\n",
       "    )\n",
       "    (norm): Gemma3RMSNorm((1152,), eps=1e-06)\n",
       "    (rotary_emb): Gemma3RotaryEmbedding()\n",
       "    (rotary_emb_local): Gemma3RotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=1152, out_features=262144, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_model = AutoModelForCausalLM.from_pretrained(base_model_id, trust_remote_code=True, torch_dtype=torch.bfloat16, cache_dir=cache_dir)\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model_id, trust_remote_code=True, cache_dir=cache_dir)\n",
    "\n",
    "base_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cfd9c209",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pad token: <pad>\n",
      "Pad token id: 0\n",
      "EOS token: <eos>\n",
      "EOS token id: 1\n"
     ]
    }
   ],
   "source": [
    "if tokenizer.pad_token is None or tokenizer.pad_token_id is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "print(f'Pad token: {tokenizer.pad_token}')\n",
    "print(f'Pad token id: {tokenizer.pad_token_id}')\n",
    "\n",
    "print(f'EOS token: {tokenizer.eos_token}')\n",
    "print(f'EOS token id: {tokenizer.eos_token_id}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "204ac4f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_config = LoraConfig(\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    "    inference_mode=False,\n",
    "    r=8,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.1,\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6dffca21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 1,038,336 || all params: 1,000,924,288 || trainable%: 0.1037\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PeftModelForCausalLM(\n",
       "  (base_model): LoraModel(\n",
       "    (model): Gemma3ForCausalLM(\n",
       "      (model): Gemma3TextModel(\n",
       "        (embed_tokens): Gemma3TextScaledWordEmbedding(262144, 1152, padding_idx=0)\n",
       "        (layers): ModuleList(\n",
       "          (0-25): 26 x Gemma3DecoderLayer(\n",
       "            (self_attn): Gemma3Attention(\n",
       "              (q_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=1152, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=1152, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (k_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=1152, out_features=256, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=1152, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=256, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (v_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=1152, out_features=256, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=1152, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=256, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (o_proj): Linear(in_features=1024, out_features=1152, bias=False)\n",
       "              (q_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
       "              (k_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
       "            )\n",
       "            (mlp): Gemma3MLP(\n",
       "              (gate_proj): Linear(in_features=1152, out_features=6912, bias=False)\n",
       "              (up_proj): Linear(in_features=1152, out_features=6912, bias=False)\n",
       "              (down_proj): Linear(in_features=6912, out_features=1152, bias=False)\n",
       "              (act_fn): PytorchGELUTanh()\n",
       "            )\n",
       "            (input_layernorm): Gemma3RMSNorm((1152,), eps=1e-06)\n",
       "            (post_attention_layernorm): Gemma3RMSNorm((1152,), eps=1e-06)\n",
       "            (pre_feedforward_layernorm): Gemma3RMSNorm((1152,), eps=1e-06)\n",
       "            (post_feedforward_layernorm): Gemma3RMSNorm((1152,), eps=1e-06)\n",
       "          )\n",
       "        )\n",
       "        (norm): Gemma3RMSNorm((1152,), eps=1e-06)\n",
       "        (rotary_emb): Gemma3RotaryEmbedding()\n",
       "        (rotary_emb_local): Gemma3RotaryEmbedding()\n",
       "      )\n",
       "      (lm_head): Linear(in_features=1152, out_features=262144, bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "peft_model = get_peft_model(base_model, peft_config)\n",
    "peft_model.print_trainable_parameters()\n",
    "peft_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f4d42bab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # dataset = load_dataset(\"philschmid/dolly-15k-oai-style\")\n",
    "# dataset = load_dataset(\"MBZUAI/Bactrian-X\", \"en\")\n",
    "\n",
    "# for split in dataset:\n",
    "#     dataset[split] = dataset[split].select(range(10))\n",
    "#     print(f\"{split}: {len(dataset[split])}\")\n",
    "\n",
    "# # dataset =\n",
    "\n",
    "# dataset, dataset[\"train\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2e6d9d4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Unnamed: 0': 0, 'question': 'hi, how are you doing?', 'answer': \"i'm fine. how about yourself?\"}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from datasets import Dataset\n",
    "\n",
    "# Load JSONL into list of dicts\n",
    "with open(\"dataset/chatbot/train.jsonl\", \"r\", encoding=\"utf-8\") as f:\n",
    "    raw_data = [json.loads(line) for line in f]\n",
    "\n",
    "# Create Hugging Face Dataset\n",
    "dataset = Dataset.from_list(raw_data)\n",
    "\n",
    "# Optional: Check a sample\n",
    "print(dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "939af9ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 3225/3225 [00:01<00:00, 2590.31 examples/s]\n"
     ]
    }
   ],
   "source": [
    "INSTRUCTION_TEMPLATE_WITH_INPUT = \"\"\"### Instruction:\n",
    "Chat and fullfil request of user\n",
    "\n",
    "### Input:\n",
    "{input}\n",
    "\n",
    "### Response:\n",
    "\"\"\"\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    results = {\n",
    "        \"input_ids\": [],\n",
    "        \"labels\": [],\n",
    "        \"attention_mask\": [],\n",
    "    }\n",
    "\n",
    "    for i in range(len(examples['question'])):\n",
    "        cur_input = examples['question'][i]\n",
    "        cur_output = examples['answer'][i]\n",
    "\n",
    "        cur_prompt = INSTRUCTION_TEMPLATE_WITH_INPUT.format(input=cur_input)\n",
    "\n",
    "        cur_prompt_tokenized = tokenizer(cur_prompt, return_tensors=\"pt\", add_special_tokens=True)\n",
    "        cur_output_tokenized = tokenizer(cur_output, return_tensors=\"pt\", add_special_tokens=False)\n",
    "\n",
    "        cur_prompt_ids = cur_prompt_tokenized[\"input_ids\"][0]\n",
    "        cur_output_ids = cur_output_tokenized[\"input_ids\"][0]\n",
    "        cur_output_ids = torch.cat([cur_output_ids, torch.tensor([tokenizer.eos_token_id])], dim=0)\n",
    "\n",
    "        input_ids = torch.cat([cur_prompt_ids, cur_output_ids], dim=0)\n",
    "        label = torch.cat([torch.full_like(cur_prompt_ids, fill_value=-100), cur_output_ids], dim=0)\n",
    "\n",
    "        results[\"input_ids\"].append(input_ids)\n",
    "        results[\"labels\"].append(label)\n",
    "        results['attention_mask'].append(torch.ones_like(input_ids))\n",
    "\n",
    "    return results\n",
    "\n",
    "tokenized_dataset = dataset.map(\n",
    "    tokenize_function,\n",
    "    batched=True,\n",
    "    remove_columns=dataset.column_names,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "053d91e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [2, 10354, 64017, 236787, 107, 20535, 532, 2587, 3267, 2864, 529, 2430, 108, 10354, 13065, 236787, 107, 2202, 236764, 1217, 659, 611, 3490, 236881, 108, 10354, 14503, 236787, 107, 236747, 236789, 236757, 5851, 236761, 1217, 1003, 5869, 236881, 1], 'labels': [-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 236747, 236789, 236757, 5851, 236761, 1217, 1003, 5869, 236881, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "<bos>### Instruction:\n",
      "Chat and fullfil request of user\n",
      "\n",
      "### Input:\n",
      "hi, how are you doing?\n",
      "\n",
      "### Response:\n",
      "i'm fine. how about yourself?<eos>\n"
     ]
    }
   ],
   "source": [
    "print(tokenized_dataset[0])\n",
    "print(tokenizer.decode(tokenized_dataset[0]['input_ids'], skip_special_tokens=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9de7d423",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./output\",\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=2,\n",
    "    save_steps=2,\n",
    "    max_steps=200,\n",
    "    save_total_limit=2,\n",
    "    report_to=\"none\",\n",
    "    push_to_hub=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "85854fbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorWithPadding\n",
    "from typing import Any, Dict, List\n",
    "\n",
    "class RightPaddingDataCollator(DataCollatorWithPadding):\n",
    "    \"\"\"The default data collator pads only inputs, not including the labels.\"\"\"\n",
    "\n",
    "    def __init__(self, tokenizer, max_length: int = 1024):\n",
    "        super().__init__(tokenizer, max_length=max_length)\n",
    "\n",
    "    def __call__(self, features: List[Dict[str, Any]]) -> Dict[str, Any]:\n",
    "        input_ids, labels, attention_mask = [], [], []\n",
    "        max_batch_len = max(len(f[\"input_ids\"]) for f in features)\n",
    "\n",
    "        for sample in features:\n",
    "            # Convert to torch tensors\n",
    "            cur_input_ids = torch.tensor(sample[\"input_ids\"], dtype=torch.long)\n",
    "            cur_labels = torch.tensor(sample[\"labels\"], dtype=torch.long)\n",
    "            cur_attention_mask = torch.ones_like(cur_input_ids)\n",
    "\n",
    "            # Next, we pad the inputs and labels to the maximum length within the batch\n",
    "            pad_token_id = self.tokenizer.pad_token_id\n",
    "            padding_length = max_batch_len - len(cur_input_ids)\n",
    "            cur_input_ids = torch.cat([cur_input_ids, torch.full((padding_length,), fill_value=pad_token_id, dtype=torch.long)])\n",
    "            cur_labels = torch.cat([cur_labels, torch.full((padding_length,), fill_value=-100, dtype=torch.long)])\n",
    "            cur_attention_mask = torch.cat([cur_attention_mask, torch.zeros((padding_length,), dtype=torch.long)])\n",
    "\n",
    "            # Truncate the inputs and labels to the maximum length\n",
    "            cur_input_ids = cur_input_ids[:max_batch_len]\n",
    "            cur_labels = cur_labels[:max_batch_len]\n",
    "            cur_attention_mask = cur_attention_mask[:max_batch_len]\n",
    "\n",
    "            # Append to the return lists\n",
    "            input_ids.append(cur_input_ids)\n",
    "            labels.append(cur_labels)\n",
    "            attention_mask.append(cur_attention_mask)\n",
    "\n",
    "        # Return formatted batch.\n",
    "        return {\n",
    "            \"input_ids\": torch.stack(input_ids),\n",
    "            \"labels\": torch.stack(labels),\n",
    "            \"attention_mask\": torch.stack(attention_mask)\n",
    "        }\n",
    "\n",
    "\n",
    "data_collator = RightPaddingDataCollator(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "80641f38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[    2, 10354, 64017,  ...,     0,     0,     0],\n",
       "         [    2, 10354, 64017,  ...,     0,     0,     0],\n",
       "         [    2, 10354, 64017,  ...,     0,     0,     0],\n",
       "         ...,\n",
       "         [    2, 10354, 64017,  ...,     0,     0,     0],\n",
       "         [    2, 10354, 64017,  ...,     0,     0,     0],\n",
       "         [    2, 10354, 64017,  ...,     0,     0,     0]]),\n",
       " 'labels': tensor([[-100, -100, -100,  ..., -100, -100, -100],\n",
       "         [-100, -100, -100,  ..., -100, -100, -100],\n",
       "         [-100, -100, -100,  ..., -100, -100, -100],\n",
       "         ...,\n",
       "         [-100, -100, -100,  ..., -100, -100, -100],\n",
       "         [-100, -100, -100,  ..., -100, -100, -100],\n",
       "         [-100, -100, -100,  ..., -100, -100, -100]]),\n",
       " 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0]])}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_collator(tokenized_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d87283f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n",
      "It is strongly recommended to train Gemma3 models with the `eager` attention implementation instead of `sdpa`. Use `eager` with `AutoModelForCausalLM.from_pretrained('<path-to-checkpoint>', attn_implementation='eager')`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='200' max='200' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [200/200 02:20, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.669700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>2.859200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>2.432700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>2.622000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>2.436100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>1.834900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>2.088300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>2.749000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>2.976200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>2.495600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>1.727800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>2.652700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>1.930200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>2.526600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>1.750500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>1.978200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>1.330900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>1.541000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>1.671400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>1.848200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42</td>\n",
       "      <td>1.786900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44</td>\n",
       "      <td>1.779800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46</td>\n",
       "      <td>1.494400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48</td>\n",
       "      <td>1.674400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>1.379200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52</td>\n",
       "      <td>1.568600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>54</td>\n",
       "      <td>1.642800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>56</td>\n",
       "      <td>1.024700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>58</td>\n",
       "      <td>1.355100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>1.756700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>62</td>\n",
       "      <td>1.181200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>64</td>\n",
       "      <td>1.408000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>66</td>\n",
       "      <td>1.253100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>68</td>\n",
       "      <td>1.120900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>1.327500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>72</td>\n",
       "      <td>1.304100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>74</td>\n",
       "      <td>1.415100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>76</td>\n",
       "      <td>1.286600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>78</td>\n",
       "      <td>1.323100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>1.391100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>82</td>\n",
       "      <td>1.680300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>84</td>\n",
       "      <td>1.071600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>86</td>\n",
       "      <td>1.499800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>88</td>\n",
       "      <td>1.622200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>1.193300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>92</td>\n",
       "      <td>1.500700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>94</td>\n",
       "      <td>1.144300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>96</td>\n",
       "      <td>1.391900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>98</td>\n",
       "      <td>1.302600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.447100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>102</td>\n",
       "      <td>1.365400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>104</td>\n",
       "      <td>1.143400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>106</td>\n",
       "      <td>1.154700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>108</td>\n",
       "      <td>1.532900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>1.505100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>112</td>\n",
       "      <td>1.381900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>114</td>\n",
       "      <td>1.323500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>116</td>\n",
       "      <td>1.109400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>118</td>\n",
       "      <td>1.024200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>1.204500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>122</td>\n",
       "      <td>1.565200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>124</td>\n",
       "      <td>1.392300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>126</td>\n",
       "      <td>1.203600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>128</td>\n",
       "      <td>1.338700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>1.329200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>132</td>\n",
       "      <td>1.118800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>134</td>\n",
       "      <td>1.377500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>136</td>\n",
       "      <td>1.643900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>138</td>\n",
       "      <td>1.104000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>1.186900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>142</td>\n",
       "      <td>1.473700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>144</td>\n",
       "      <td>1.449200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>146</td>\n",
       "      <td>1.008200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>148</td>\n",
       "      <td>1.420000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>1.559200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>152</td>\n",
       "      <td>1.261700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>154</td>\n",
       "      <td>1.529400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>156</td>\n",
       "      <td>1.227600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>158</td>\n",
       "      <td>1.415900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>1.588400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>162</td>\n",
       "      <td>1.291700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>164</td>\n",
       "      <td>1.253000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>166</td>\n",
       "      <td>1.399900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>168</td>\n",
       "      <td>2.067500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>170</td>\n",
       "      <td>1.279200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>172</td>\n",
       "      <td>1.678700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>174</td>\n",
       "      <td>1.360800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>176</td>\n",
       "      <td>1.216700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>178</td>\n",
       "      <td>1.161500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>1.341700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>182</td>\n",
       "      <td>1.120400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>184</td>\n",
       "      <td>1.249100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>186</td>\n",
       "      <td>1.084500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>188</td>\n",
       "      <td>1.109700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>190</td>\n",
       "      <td>1.310600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>192</td>\n",
       "      <td>1.263500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>194</td>\n",
       "      <td>1.459900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>196</td>\n",
       "      <td>0.994100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>198</td>\n",
       "      <td>1.205800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>1.601300</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=200, training_loss=1.5283658808469773, metrics={'train_runtime': 141.4456, 'train_samples_per_second': 11.312, 'train_steps_per_second': 1.414, 'total_flos': 329919402700800.0, 'train_loss': 1.5283658808469773, 'epoch': 0.49504950495049505})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from trl import SFTTrainer\n",
    "from transformers import Trainer\n",
    "\n",
    "# trainer = Trainer(\n",
    "trainer = Trainer(\n",
    "    model=peft_model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset,\n",
    "    eval_dataset=None,\n",
    "    data_collator=data_collator\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
